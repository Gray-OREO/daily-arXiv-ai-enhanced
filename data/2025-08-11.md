<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.LG](#cs.LG) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新的对抗攻击方法ResPA，通过利用残差梯度引导对抗样本向损失函数的平坦区域移动，从而提高了对抗样本的可转移性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于转移的对抗攻击方法在损失函数平坦区域的对抗样本表现出更强的可转移性，但忽视了扰动方向的影响，导致可转移性受限。

Method: 提出ResPA方法，利用残差梯度作为扰动方向，通过指数移动平均获取历史梯度的参考梯度，并计算当前梯度与参考梯度的残差来捕捉全局扰动方向的变化。

Result: 实验结果表明，ResPA的可转移性优于现有的典型转移攻击方法，且与当前输入变换方法结合可进一步提升可转移性。

Conclusion: ResPA通过优化扰动方向，显著提高了对抗样本的可转移性，为黑盒攻击场景提供了一种更有效的方法。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: 本文提出了一种广义的少样本离群检测（GOOD）框架，通过通用知识模型提升检测模型的泛化能力，并提出了知识动态嵌入（KDE）机制以动态调整知识指导。实验证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有少样本离群检测方法在开放世界中的泛化能力不足，容易过拟合，性能不稳定。本文旨在解决这一问题。

Method: 提出GOOD框架，使用辅助通用知识模型（GKM）增强检测模型的泛化能力；提出KDE机制动态调整知识指导；提出GS平衡理论降低泛化误差。

Result: 在真实离群检测基准测试中表现优越。

Conclusion: GOOD框架和KDE机制有效提升少样本离群检测的泛化能力，GS平衡理论为泛化性能提供了理论支持。

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [3] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide是一种新颖的方法，通过动态推理机制UnGuidance，结合LoRA适配器，实现精确的模型遗忘，同时保持不相关内容的保真度。


<details>
  <summary>Details</summary>
Motivation: 大型文本到图像扩散模型可能被滥用于生成有害或误导性内容，因此需要有效的方法在不影响整体性能的情况下移除特定知识。

Method: UnGuide结合了LoRA适配器和Classifier-Free Guidance（CFG）的动态推理机制，通过调整指导尺度实现选择性遗忘。

Result: UnGuide在概念移除任务中表现优异，既能有效移除目标内容，又能保持图像的真实感和模型的表达能力。

Conclusion: UnGuide为解决扩散模型中的选择性遗忘问题提供了一种高效且精确的解决方案，优于现有的LoRA方法。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [4] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 提出了一个基于部分卷积的风格迁移网络，专注于对图像中特定区域进行风格迁移，避免了传统后处理掩码方法的缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统方法对整个图像进行风格迁移后通过掩码处理特定区域，但会错误捕获风格特征，因此需要一种更精确的区域风格迁移方法。

Method: 采用部分卷积风格迁移网络，结合网络内部混合技术，以改善区域选择的不完美问题。

Result: 在SA-1B数据集上的实验表明，该方法在视觉和定量上均优于传统方法。

Conclusion: 该方法能够准确地将风格特征应用于感兴趣区域，同时通过内部混合技术提升了整体效果，代码已公开。

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [5] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2是一种加速的3D医学图像合成框架，通过集成整流流实现快速高质量生成，并引入区域特异性对比损失提高条件一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在泛化性、推理速度和输入条件对齐方面的不足。

Method: 整合整流流加速生成，并提出区域特异性对比损失增强条件一致性。

Result: 实现33倍加速并达到SOTA图像质量，下游分割实验验证合成图像可用于数据增强。

Conclusion: MAISI-v2在速度和图像质量上表现出色，且有潜力推动医学图像合成领域的进一步发展。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [6] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种用于医学影像的少样本学习框架，通过预训练的MRI变换器（MAE）实现高效任务迁移。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中标注数据稀缺问题，提升变换器模型在真实世界中的应用性。

Method: 采用MAE预训练策略，结合多尺度CNN特征（MAE-FUnet）进行高低任务处理。

Result: 在分类和分割任务中表现优越，尤其适合资源有限的环境。

Conclusion: 该框架高效、稳定且可扩展，适用于低资源临床和神经影像应用。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [7] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 提出了一种无需重建或优化的3D高斯泼溅风格迁移方法，通过生成图结构并插值实现快速风格化。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D高斯泼溅风格迁移需要重建或微调的问题。

Method: 生成高斯泼溅隐含表面的图结构，应用前馈式表面风格化方法并插值回泼溅。

Result: 无需额外训练或优化，快速实现风格迁移（消费级硬件下2分钟内）。

Conclusion: 该方法在速度和质量上优于其他3D高斯泼溅风格迁移方法。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [8] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: Multi-Zoom Enhanced NeRF (MZEN) 是一种能够处理多缩放图像的 NeRF 框架，通过增强相机模型和新型姿态策略，显著提升工业检测中的细节重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有 NeRF 方法在工业检测中无法捕捉细微结构，尤其是在固定传感器分辨率和有限计算资源下。多缩放图像会破坏多视图一致性，需要新框架来解决这一问题。

Method: MZEN 通过 (i) 在相机模型中引入可学习的缩放因子，(ii) 提出一种新型姿态策略，即先解决宽场图像以建立全局坐标系，再通过缩放一致的裁剪匹配方法对齐缩放图像，最后联合优化。

Result: 在八个场景中，MZEN 显著优于无姿态基线和高分辨率变体，PSNR 提升高达 28%，SSIM 提升 10%，LPIPS 降低高达 222%。

Conclusion: MZEN 成功将 NeRF 扩展到工业场景，在保持全局精度的同时捕捉微米级细节。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [9] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: 提出TSMS-SAM2框架，通过多时序尺度采样和内存分割剪枝机制优化手术视频中的可提示对象分割与跟踪，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 因手术视频中复杂运动动态和内存冗余问题，现有SAM2模型在手术视频分析中的应用效果有限，亟需改进。

Method: 引入多时序尺度视频采样增强和内存分割剪枝机制，分别应对运动变异性与内存冗余问题。

Result: 在EndoVis2017和EndoVis2018数据集上，TSMS-SAM2的Dice分数分别达到95.24和86.73，优于现有方法。

Conclusion: TSMS-SAM2通过创新策略有效解决了手术视频分割的挑战，展现了在复杂场景中的潜力。

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [10] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 提出了一种名为TCA的轻量级策略，通过利用视频中时间连贯性优化令牌聚类，显著降低计算成本同时保留细节。


<details>
  <summary>Details</summary>
Motivation: 解决Swin Transformer在视频分割中计算成本过高的问题，同时利用时间冗余进一步提升性能。

Method: 引入Temporal Cluster Assignment (TCA)，基于时间相关性优化令牌聚类，避免无区别丢弃冗余令牌。

Result: 在多个数据集上验证了TCA能显著提高现有方法的精度与速度权衡，适用于自然和领域特定视频。

Conclusion: TCA是一种无需微调的有效策略，可广泛应用于视频分割任务。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [11] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉-语言的框架，通过自然语言建模驾驶员的注视变化，利用少样本和零样本学习提升自动驾驶中的注意力预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态图像的注意力分配，而本文旨在通过语言描述动态预测驾驶员的注视变化，提升自动驾驶的可解释性。

Method: 基于BDD-A数据集优化高质量标注，微调LLaVA模型，结合低层和高层上下文（如路线语义、风险预测）生成语言驱动的注视行为描述。

Result: 微调模型在注意力转移检测和可解释性上优于通用视觉-语言模型，并通过领域特定指标验证了语义对齐和响应多样性。

Conclusion: 该研究为自动驾驶中的可解释AI提供了新方向，支持下游任务（如行为预测、人机协作等）。

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [12] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了一种利用多摄像头视角进行注视目标估计（GTE）任务的方法，通过整合不同视角的信息来提高准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有单视角方法在面临人脸遮挡、目标模糊和目标超出视野等挑战时表现受限，因此需要一种多视角方法来解决这些问题。

Method: 该方法通过处理一对摄像头视角输入，结合头部信息聚合（HIA）模块、基于不确定性的注视选择（UGS）模块和基于极线的场景注意力（ESA）模块，实现跨视角背景信息共享和更准确的注视估计。

Result: 该方法显著优于单视角基线，尤其是在第二摄像头提供清晰人脸视图时。此外，其还能仅通过第二视角图像估计第一视角的注视目标，这是单视角方法无法实现的。

Conclusion: 本文提出的多视角GTE方法不仅提高了准确性和适用性，还引入了多视角数据集，为未来研究提供了支持。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [13] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA提出了一种高效测试时适应方法，通过递归更新模块和自适应集成模块，动态优化决策边界并减少对提示的依赖，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型（如CLIP）在分布偏移下泛化能力不足，测试时适应（TTA）虽能解决此问题，但现有缓存方法仅存储高置信度样本，限制了决策边界的优化。

Method: ETTA引入递归更新模块（动态整合所有测试样本以优化决策边界）和自适应集成模块（动态选择最优提示以减少提示依赖），并基于置信度结合两者得分。

Result: 在两个基准测试上，ETTA在计算复杂度和准确性上均超越现有TTA模型，建立了高效测试时适应的新标准。

Conclusion: ETTA通过动态更新和集成策略显著提升了测试时适应效率与准确性，为相关研究提供了新方向。

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [14] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一种先进的视觉语言引导框架，用于生成3D场景并支持基于人类反馈的交互式编辑，能够根据细粒度文本描述生成多样化的高质量场景。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景生成依赖大量人工操作，且现有自动化方法难以生成开放域场景或支持灵活编辑，因此需要一种能从文本直接生成高质量3D世界的解决方案。

Method: HOLODECK 2.0通过视觉语言模型解析场景需求并生成高质量3D资产，借助空间约束实现语义连贯且物理合理的布局，支持迭代编辑和人类反馈。

Result: 人类评估和CLIP测试显示，HOLODECK 2.0在生成与文本描述高度一致的场景方面优于基线模型，并能灵活适应室内和开放域环境。

Conclusion: HOLODECK 2.0在3D场景生成和编辑方面表现优异，尤其在游戏建模中有潜力提升效率。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [15] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为RopStitch的无监督深度图像拼接框架，兼顾鲁棒性和自然性。通过双分支架构融入内容感知的通用先验，并利用虚拟最优平面缓解内容对齐与结构保留的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统的图像拼接方法在鲁棒性和自然性方面存在局限，尤其是在多样化的真实场景中表现不佳。本文旨在解决这一问题。

Method: 采用双分支架构分别捕获粗粒度与细粒度特征，并通过虚拟最优平面的概念和同构分解系数迭代预测优化拼接效果。

Result: 实验表明，RopStitch在多个数据集上显著优于现有方法，尤其在场景鲁棒性和内容自然性方面表现突出。

Conclusion: RopStitch通过创新的双分支架构和虚拟最优平面设计，成功解决了图像拼接中的鲁棒性和自然性问题，具有广泛应用潜力。

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [16] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: 该论文探讨了利用神经场模型直接从智能手机拍摄的图像数据中高效重建复杂场景几何和光照效果，避免了传统方法中的预处理和标注数据需求。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用现代智能手机的多传感器和计算能力，结合神经场模型的优势，开发出更高效、无需复杂预处理的计算成像方法。

Method: 论文提出了一种精心设计的神经场模型，通过随机梯度下降直接拟合智能手机的原始测量数据，实现了深度估计、图层分离和图像拼接等应用。

Result: 所提方法在无需复杂预处理或标注数据的情况下，超越了现有最先进方法的表现。

Conclusion: 研究表明，神经场模型与智能手机计算能力的结合，为计算成像提供了高效、简洁的解决方案。

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [17] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 该论文评估了两种3D分割方法（SAM和Mask3D）在复杂建筑工地环境中的适应性，指出当前方法在户外场景中的不足，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统建筑进度监测方法效率低下，且难以应对复杂多变的建筑工地环境，计算机视觉方法被探索以提高效率和可扩展性。

Method: 研究比较了两种先进3D分割方法（SAM和Mask3D）在室内和室外建筑工地环境中的性能，并评估其适应性。

Result: 研究发现当前分割方法缺乏户外场景的基准测试，SAM和Mask3D在建筑工地环境中的表现揭示了现有方法的局限性。

Conclusion: 研究强调了开发针对建筑工地数据的定制化分割工作流程的必要性，以推动自动化监测技术的发展。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [18] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD框架通过3D高斯分布引导的扩散模型，自监督解决单图像法线估计中的多视角不一致和数据依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据驱动的统计先验，缺乏光表面相互作用的显式建模，导致多视角法线方向冲突和梯度不连续。

Method: 结合物理驱动的光相互作用建模和可微分渲染重投影策略，通过3D高斯重参数化模型和跨域特征融合模块优化法线生成。

Result: 在Google Scanned Objects数据集上表现优于现有方法。

Conclusion: SINGAD通过自监督优化和多视角一致性建模，显著提升了单图像法线估计的精度和鲁棒性。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [19] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1是一个统一框架，通过利用patch级CLIP图像嵌入作为潜在变量，将预训练的多模态大语言模型（MLLMs）与扩散模型结合，实现高保真可控图像生成，同时显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接在LLMs上训练或桥接LLMs与扩散模型时，由于LLMs在预训练阶段未见过图像表示，导致训练成本高昂。Bifrost-1旨在解决这一问题，同时保持多模态推理能力。

Method: 通过patch级CLIP图像嵌入作为潜在变量，轻量级适配ControlNet，并为MLLM初始化视觉生成分支来预测patch级嵌入。

Result: Bifrost-1在视觉保真度和多模态理解方面优于或媲美现有方法，且训练计算成本显著降低。

Conclusion: Bifrost-1框架有效整合了预训练MLLMs与扩散模型，实现了高效可控的图像生成，并通过消融实验验证了设计的有效性。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [20] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG框架通过几何基元提取和语义锚定，解决了机器人操作中语义与几何特征的割裂问题，实现了动态语义-功能关系的建模，并在实际任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中高层次任务语义与低层次几何特征之间的割裂问题，以及现有视觉语言模型（VLMs）在语义锚定和动态关系捕捉上的不足。

Method: 提出PASG框架，包括自动几何基元提取、VLM驱动的语义锚定以及空间语义推理基准和微调VLM（Qwen2.5VL-PA）。

Result: PASG在多样化的机器人操作任务中表现出色，性能接近人工标注效果。

Conclusion: PASG为机器人操作提供了一个统一的语义-几何基元范式，实现了更细粒度的语义-功能理解。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [21] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene提出了一种统一框架，解决3D场景重建与4D人物动画无缝整合的挑战。


<details>
  <summary>Details</summary>
Motivation: 将3D场景重建与4D人物动画无缝整合以生成视觉吸引力的结果仍具有挑战性。

Method: 设计精确放置模块防止穿模，提出无训练风格对齐方法，开发联合后重建方法支持相机轨迹。

Result: AnimateScene在各种相机和动作组合下生成了高几何细节和时空一致性的动态场景视频。

Conclusion: AnimateScene通过一体化方法解决了整合问题，并取得了显著效果。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [22] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 提出了一种基于能量的测试时适应方法（ETA），用于调整预训练的深度补全模型在新环境中的预测，通过对抗扰动探索数据空间，并在三个室内和三个室外数据集中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 深度补全模型在从源数据转移到目标数据时，由于协变量偏移常导致预测错误。目标在于无需预知目标数据分布的情况下，适应新环境。

Method: 利用对抗扰动探索数据空间，训练能量模型评估预测的分布情况，并在测试时更新模型参数以最小化能量，使预测与源分布对齐。

Result: ETA在三个室内和室外数据集中平均比现有方法分别提升10.23%和6.94%。

Conclusion: ETA通过测试时适应有效缓解协变量偏移问题，显著提升深度补全模型在新环境中的性能。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [23] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 提出了一种高效的视频计算机视觉系统，通过去除图像信号处理器和优化运动估计算法，显著减少计算冗余并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频计算机视觉系统未能完全减少时间冗余，且忽视了前端计算开销，因此需要一种更高效的解决方案。

Method: 1. 移除图像信号处理器，直接将Bayer格式数据输入模型；2. 提出基于快速块匹配的运动估计算法，并结合MV细化模块和上下文感知块细化网络校正误差；3. 采用帧选择策略平衡精度与效率。

Result: 在多个视频计算机视觉任务中，该方法显著加速计算，仅带来轻微的性能损失。

Conclusion: 该方法通过优化数据输入和运动估计算法，实现了高效且精度损失较小的视频计算机视觉系统。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [24] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态情感识别框架，利用预训练模型提取视觉、音频和文本特征，并通过自注意力机制和残差连接进行融合，最终在MER2025-SEMI数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互中的情感识别能力，解决数据稀缺问题。

Method: 设计双分支视觉编码器、上下文增强的文本处理方法和多模态融合策略，结合自注意力机制和残差连接。

Result: 加权F-score从78.63%提升至87.49%，显著优于基线。

Conclusion: 提出的框架在多模态情感识别任务中表现高效，验证了其有效性。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [25] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: 论文提出了MakeupQuad数据集和EvoMakeup框架，用于高质量、可控的面部化妆编辑，解决了现有方法在细节和保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的面部化妆编辑方法由于缺乏配对数据，导致编辑效果粗糙，难以同时保持身份和化妆保真度。

Method: 引入MakeupQuad数据集并提出EvoMakeup框架，通过多阶段蒸馏避免图像质量下降，实现数据和模型的迭代优化。

Result: EvoMakeup在真实场景中表现优异，支持高保真、可控的多任务化妆编辑，包括全脸、局部和文本驱动编辑。

Conclusion: 该方法在化妆保真度和身份保持上达到了更好的平衡，代码和数据集将在论文接受后公开。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [26] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: 引入MathReal数据集，评估MLLM在真实教育场景中的数学推理能力，发现现有模型面临显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准多基于干净或处理过的多模态输入，缺乏真实K-12教育场景中的图像数据。

Method: 构建MathReal数据集（2000个问题），分类真实图像问题，设计6种实验设置评估MLLM性能。

Result: 现有MLLM在真实教育场景中的解题能力显著受限，分析其错误模式并提出改进方向。

Conclusion: MathReal揭示了MLLM在真实场景中的不足，为未来研究提供了改进方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [27] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯溅射的管道，通过生成额外训练视图和虚拟相机策略，提升渲染质量，并在Wild-Explore基准上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在偏离训练轨迹的视角下渲染时存在伪影和缺失区域的问题，限制了无缝场景探索。

Method: 采用信息增益驱动的虚拟相机放置策略和视频扩散先验优化渲染结果，并通过增强视图微调3D高斯模型。

Result: 实验表明，该方法优于现有3DGS方法，实现了任意视角下的高质量无伪影渲染。

Conclusion: 该方法显著提升了重建质量，为场景探索提供了更优的解决方案。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [28] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 利用深度学习与流式成像显微镜结合分析亚可见颗粒，可区分无害成分（如硅油）与蛋白质颗粒。但数据稀缺和类别不平衡问题限制了多分类器的效果。本文提出一种先进的扩散模型生成高保真图像以增强数据集，提升分类器性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决亚可见颗粒分析中数据稀缺和类别不平衡问题，尤其是硅油和气泡等难以获取数据的颗粒类型。

Method: 开发扩散模型生成高保真图像以增强训练数据集，并训练多类别深度神经网络进行分类。

Result: 生成的图像在视觉质量和结构上与真实图像相似，实验表明该方法能显著提升分类性能且无显著负面影响。

Conclusion: 扩散模型生成的图像能有效解决数据不平衡问题，提升分类器性能，相关模型和代码已开源以促进研究。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [29] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum提出了一种新方法，通过利用图像到纹理扩散模型，实现更精细的人体部分和服装解析，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体解析中使用固定类别标签，无法区分细粒度的服装类型；而现有的开放词汇分割方法无法区分多样化的服装或详细的身体部位。

Method: Spectrum通过微调文本到图像扩散模型，提出了一种基于图像到纹理扩散模型的网络，用于提取人体部分的内部特征并生成语义有效的掩码。

Result: 实验表明，Spectrum在不同数据集上均优于基线方法，能够准确解析身体部位和服装类别。

Conclusion: Spectrum通过结合扩散模型和细粒度解析，显著提升了人体和服装分割的性能。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [30] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit是一种基于RectifiedFlow框架的快速文本引导图像编辑方法，通过PerRFI反演策略和Inversion Latent Injection技术，实现了高效且一致的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本引导图像编辑方法在快速性和编辑质量上的不足，特别是在保持关键内容的同时紧密遵循文本指令的需求。

Method: 采用RectifiedFlow框架，引入PerRFI反演策略、Inversion Latent Injection再生方法和Disentangled Prompt Guidance技术，结合Canny-conditioned ControlNet以增强结构线索。

Result: 在PIE数据集上的实验表明，InstantEdit不仅速度快，而且在定性和定量结果上优于现有少步编辑方法。

Conclusion: InstantEdit通过创新技术实现了快速、高质量的图像编辑，为文本引导编辑任务提供了有效的解决方案。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [31] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种半监督学习框架，通过融合多模态输入和共识伪标记策略，在MER2025-SEMI挑战赛中取得了第二名的成绩。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习赛道(MER-SEMI)中的情感识别问题，特别是在未标记数据的利用和模型性能提升方面。

Method: 1. 构建混合专家系统(MoE)，整合多模态输入（如视觉-语言模型和动作单元信息）。
2. 提出基于共识的伪标记策略，结合基线模型和Gemini生成高质量标签。
3. 采用两阶段训练和多专家投票集成，结合基于规则的重新排序以修正预测偏差。

Result: 在MER2025-SEMI测试集上F1得分为0.8772，排名第二。

Conclusion: 所提出的框架通过多模态融合和伪标记策略显著提升了情感识别性能，且代码已开源。

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [32] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM提出了一种在频域压缩视觉特征的高效方法，显著减少计算开销和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）中大量视觉标记增加了上下文长度，导致高计算开销和推理延迟。

Method: 通过离散余弦变换（DCT）和快速傅里叶变换（FFT）对视觉特征进行低通滤波压缩。

Result: 实验显示，Fourier-VLM在多个基准测试中表现优异，推理FLOPs减少83.8%，生成速度提升31.2%。

Conclusion: Fourier-VLM在保持性能的同时显著提升了效率和实用性。

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [33] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 该论文提出了一种基于自回归图像生成的Next Editing-token Prediction（NEP）方法，用于文本引导的图像编辑，仅重新生成需要编辑的区域，从而减少计算成本并提升编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本引导图像编辑方法通常生成整个目标图像，而非选择性编辑局部区域，导致不必要的计算成本和编辑质量下降。

Method: 作者提出了NEP框架，基于预训练的自回归文本到图像模型，仅重新生成需要编辑的区域，并支持零样本图像编辑。

Result: 该方法在广泛使用的图像编辑基准测试中取得了最新最优结果，并支持通过零样本方式进行测试时迭代优化。

Conclusion: NEP方法通过选择性编辑区域，显著提高了文本引导图像编辑的效率和效果，同时支持灵活的零样本应用。

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [34] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出VQAThinker，一种基于推理的视频质量评估框架，结合多模态模型与强化学习，解决了现有模型泛化性和可解释性不足的问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估模型在泛化性和可解释性上的局限性限制了其实际应用，亟需改进。

Method: 采用GRPO强化学习算法，结合三种奖励机制（回归、排序、时间一致性），模拟人类决策过程。

Result: VQAThinker在域内和域外基准测试中表现最优，同时在质量描述和失真归因任务上优于现有模型。

Conclusion: 强化学习结合多模态模型是构建泛化性强且可解释的视频质量评估模型的有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [35] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net通过变形联合模板生成3D LV网格，提升了MRI下的心室形状分析准确性和鲁棒性，并在阿尔茨海默病研究中发现显著相关的LV子区域。


<details>
  <summary>Details</summary>
Motivation: 侧脑室(LV)形状分析可作为神经疾病的生物标志物，但由于个体形状差异大和MRI分辨率限制，现有方法存在挑战。

Method: LV-Net通过解剖学感知的联合LV-海马模板网格变形生成个体化3D LV网格，并结合顶点分类提升点对应性。

Result: LV-Net在重建精度和形状统计上表现优越，并在阿尔茨海默病分析中识别出显著相关的LV子区域。

Conclusion: LV-Net为LV形状分析提供了高效可靠的工具，并展示了在疾病研究中的潜力。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [36] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 论文讨论了卫星光谱图像作为AGI中未被充分关注的模态的重要性，并提出了一个更全面的基准测试框架来评估地球观测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态数据在AGI研究中备受关注，但卫星光谱图像仍未得到足够重视，其潜力尚未被充分挖掘。

Method: 论文通过论证地球观测数据的价值，回顾现有基准测试并指出其局限性，提出了一套新的任务集以构建更全面的评估标准。

Result: 强调了卫星光谱图像对AGI的价值，并提出了改进基准测试的方向。

Conclusion: 需要一个更全面的基准测试来推动地球观测模型的发展，以更好地支持AGI对自然世界的理解。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [37] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet是一个轻量级的两阶段网络，通过状态空间增强交叉注意力，解决了HybridEVS相机在去马赛克过程中的问题，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: HybridEVS相机在去马赛克过程中存在混叠和伪影问题，现有方法难以在资源有限的移动设备上解决这些问题，因此需要一种高效的方法。

Method: TSANet采用两阶段网络，分别处理事件像素修复和去马赛克任务，并引入轻量级Cross-Swin State Block，利用位置先验和状态空间模型增强全局依赖。

Result: TSANet在模拟和真实数据上均表现出色，PSNR和SSIM指标优于现有方法DemosaicFormer，同时参数和计算成本分别降低了1.86倍和3.29倍。

Conclusion: TSANet为移动设备上的高效图像去马赛克提供了新可能，代码已开源。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [38] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: 研究提出了一种联合学习方法SCJoint，用于显著性目标检测（SOD）和伪装目标检测（COD）任务。通过任务特定的可学习参数和共享网络结构，该方法成功平衡了两者矛盾，提高了检测效果。


<details>
  <summary>Details</summary>
Motivation: 显著性目标检测和伪装目标检测是两项密切相关但矛盾的任务。传统观点认为联合学习会干扰网络性能，但本研究提出了一种新视角，认为通过正确方法可以实现两任务的协同提升。

Method: 提出SCJoint联合学习框架，通过任务特定的可学习参数和共享网络结构，分别学习两个任务的解码分布特性；同时引入基于显著性的采样策略（SBSS）优化训练集。

Result: 实验证明，SCJoint能够显著提升两任务的性能，且训练时间更短。提出的通用网络JoNet能够同时捕获显著和伪装目标。

Conclusion: 研究挑战了传统观念，证明通过合理设计，可以联合学习矛盾任务并提升性能。SCJoint和SBSS为未来类似研究提供了新思路。

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [39] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena是一种通过视觉动画评估大型语言模型（LLM）和多模态大型语言模型（MLLM）的框架，利用点光源成像放大模型间的性能差异，提供直观反馈。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么基于静态数据集的真实得分形式，要么模糊地收集人类偏好，无法提供即时的、直观的性能差异反馈。BioMotion Arena旨在填补这一空白。

Method: 采用点光源成像技术，利用生物运动感知特性，通过成对比较评估收集45k多票数，分析人类与专家评分的一致性。

Result: 超过90%的评估模型（包括InternVL3和Claude-4系列）无法生成基本的人形点光源组，更不用说平滑且生物合理的运动。

Conclusion: BioMotion Arena可作为性能视觉化的挑战性基准，并提供灵活的评估框架，无需依赖真实数据。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [40] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出了一种利用隐式神经表示（INR）和基于小波扩散模型（WDM）的方法，从低分辨率临床MRI生成超分辨率伪健康3D形态，用于治疗滑车发育不良（TD）。


<details>
  <summary>Details</summary>
Motivation: 当前依赖低分辨率MRI和外科医生经验的手术方案存在局限性，导致效果不一致。

Method: 通过INR生成超分辨率MRI，多标签网络分割骨骼，WDM生成伪健康目标形态。

Result: 在25例TD患者中验证，显著改善了滑车角（SA）和滑车沟深度（TGD）。

Conclusion: 该方法无需CT，提供高分辨率3D形态，可作为术前蓝图，减少辐射暴露。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [41] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE 是一个基于指令的图像和视频编辑的统一模型，通过两阶段训练策略和数据合成方法提升编辑性能。


<details>
  <summary>Details</summary>
Motivation: 解决指令式视频编辑因训练数据不足而受限的问题，推动其实际应用。

Method: 提出两阶段训练（先图像后视频）和多种数据合成方法（拼贴和生成模型），并设计高效编辑框架。

Result: DreamVE 在多种编辑任务中表现优异，数据合成方法提升了模型的泛化和迁移能力。

Conclusion: DreamVE 通过统一框架和数据合成技术，显著提升了指令式图像和视频编辑的实用性和效果。

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [42] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo是一种统一的稳定蒸馏框架，结合了轨迹保留和分布匹配策略，可减少推理步骤并保持高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散或流动的视频合成模型需要多次迭代采样，计算开销大；现有蒸馏方法在少步设置下性能下降或增加伪影。

Method: 提出连续时间一致性蒸馏和双视角对齐（分布对齐与轨迹对齐），确保ODE轨迹精确保留。

Result: 在OpenVid-1M基准测试中，SwiftVideo在少步视频生成上显著优于现有方法。

Conclusion: SwiftVideo框架有效解决了少步设置下的性能问题，同时提升了视频生成效率。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [43] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为AdaptInfer的轻量级、即插即用框架，用于在视觉语言模型（VLM）中动态修剪视觉令牌，以减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在多模态推理任务（如视觉问答）中表现优异，但推理成本高，主要原因是预填充阶段需要处理大量视觉令牌。传统修剪方法依赖静态文本提示或注意力模式，未能利用推理中的动态内部信号。

Method: AdaptInfer包含两个关键创新：1）动态文本引导的修剪机制，利用层间文本-文本注意力图为视觉令牌提供软优先级评分；2）基于跨模态注意力转移的离线分析，提出更高效的修剪调度策略。

Result: 实验表明，该方法在LLaVA-1.5-7B模型上将CUDA延迟降低61.3%，同时保持92.9%的平均准确率。在相同令牌预算下，其性能优于现有最佳方法。

Conclusion: AdaptInfer通过动态修剪和优化的调度策略，显著降低了视觉语言模型的推理成本，同时保持了高准确率，具有广泛的适用性。

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [44] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP是一个基于Vision-Language Models (VLMs)的视频质量评估框架，通过共享跨模态适配器和质量级提示显著降低了计算成本并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 当前VQA方法通过预训练和微调面临语义知识不足和高计算资源消耗的问题，而VLMs展现了广泛的视觉任务潜力。

Method: 提出Q-CLIP框架，结合共享跨模态适配器和质量级提示，研究帧采样策略对性能的影响。

Result: Q-CLIP在多个VQA数据集上表现优异。

Conclusion: Q-CLIP通过创新设计和高效采样策略，为VQA任务提供了一个高性能且计算高效的解决方案。

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [45] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于情感驱动的反应运动生成方法，通过半监督情感先验和扩散模型提升反应合成的自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 情感是人际互动中的关键因素，现有运动生成框架未考虑情感影响，导致自然性不足。本文旨在解决这一问题。

Method: 采用半监督学习框架训练情感先验，并结合扩散模型生成情感驱动的反应运动，同时考虑空间互动和情感响应。

Result: 实验表明，该方法在反应生成任务上优于现有方法。

Conclusion: 提出的方法能生成多样且自然的反应运动，为互动任务提供了有效解决方案。

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [46] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的统一框架UGD-IML，用于图像篡改定位任务，显著减少了大数据集的依赖并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像篡改定位方法对大规模标注数据的依赖问题，并通过生成模型提升任务效果。

Method: 采用扩散模型学习数据分布，结合类别嵌入和参数共享设计，实现IML和CIML任务的统一。

Result: 在多个数据集上，UGD-IML在IML和CIML任务上的F1指标分别平均提升了9.66和4.36。

Conclusion: UGD-IML通过生成框架显著提升了图像篡改定位的效能和鲁棒性，为相关研究提供了新思路。

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [47] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: 论文提出了一种名为MCA的框架，通过多模态联合标签修正和多级自适应对齐策略，解决了2D-3D跨模态检索中的噪声标签问题，并在实验中获得优异表现。


<details>
  <summary>Details</summary>
Motivation: 当前跨模态检索中，噪声标签严重影响性能，现有方法容易因独立处理模态而陷入过拟合。

Method: 提出MCA框架，包含多模态联合标签修正（MJC）和多级自适应对齐（MAA）策略。

Result: MCA在传统和真实噪声数据集上均取得最优性能。

Conclusion: MCA框架能有效解决噪声标签问题，提升跨模态检索的鲁棒性和性能。

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [48] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 提出了一种自监督学习框架用于手写数学表达式识别（HMER），通过全局和局部对比损失预训练图像编码器，并引入渐进空间掩码策略训练的自监督注意力网络，无需监督即可学习语义焦点区域。实验表明，该方法在CROHME基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: HMER任务因二维结构、符号尺度变化和复杂空间关系而具有挑战性，且标注数据成本高昂。本文旨在通过自监督学习减少对标注数据的依赖。

Method: 方法包括三个步骤：(1) 使用全局和局部对比损失预训练图像编码器；(2) 通过渐进掩码策略训练自监督注意力网络；(3) 用Transformer解码器进行监督微调生成LATEX序列。

Result: 在CROHME基准测试中，该方法优于现有的自监督和全监督基线，验证了渐进注意力机制的有效性。

Conclusion: 提出的自监督学习框架和渐进注意力机制显著提升了HMER性能，展示了无需监督学习语义焦点区域的潜力。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [49] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++是一种新的训练框架，通过结合FMCS预测和任务标签，动态优化主干网络，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络因内部表示不透明而导致的解释性挑战，并弥补FMCE缺乏实验验证和闭环集成的不足。

Method: 提出FMCE-Net++框架，集成预训练的FMCE-Net作为辅助头，生成FMCS预测，结合任务标签通过表征辅助损失优化主干网络。

Result: 在多个数据集上验证，FMCE-Net++无需修改架构或额外数据即可提升性能，如ResNet-50/CIFAR-10准确率提升1.16%。

Conclusion: FMCE-Net++能有效提升现有多模型性能，验证了其可行性和有效性。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [50] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: 该论文提出了GMF-Drive框架，通过改进LiDAR表示和使用高效的状态空间模型（SSM）替代传统Transformer，显著提升了端到端自动驾驶的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的端到端自动驾驶方法依赖Transformer融合，但其二次计算复杂度和缺乏空间先验限制了高分辨率特征的有效利用，尤其在鸟瞰图（BEV）表示中表现不足。

Method: GMF-Drive采用几何增强的LiDAR表示和门控Mamba融合（GM-Fusion）架构，利用方向序列和自适应融合机制捕捉长距离依赖，同时保持线性计算复杂度。

Result: 在NAVSIM基准测试中，GMF-Drive性能显著优于DiffusionDrive，成为新的SOTA方法。

Conclusion: 研究证明了任务特定的状态空间模型（SSM）在自动驾驶任务中，能够超越通用Transformer的性能和效率。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [51] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST是一种新型无监督域适应方法，利用语言模态增强视觉模型的适应性，通过伪标签生成和不确定性估计优化分类损失，并通过多模态对比学习对齐视觉和语言特征空间。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法在复杂域偏移（如地理偏移）下表现不佳，而语言模态在此类任务中展现更强的鲁棒性。因此，引入语言模态以提升视觉模型的适应性。

Method: TRUST利用目标样本的标题生成伪标签，并提出基于CLIP相似度的不确定性估计策略以优化分类损失。同时，引入多模态软对比学习损失，通过标题引导视觉模型的对比学习。

Result: TRUST在经典（DomainNet）和复杂（GeoNet）域偏移任务中表现优异，达到新的最优性能。

Conclusion: 通过结合语言模态和视觉特征，TRUST显著提升了无监督域适应的性能，尤其在复杂域偏移场景下表现突出。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [52] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg提出了一种新颖的弱监督语义分割方法，通过多类别对比学习（MCCL）和特征协同结构（FSS），显著提升了开放词汇场景下的语义定位和判别能力。


<details>
  <summary>Details</summary>
Motivation: 开放词汇场景下的语义分割面临语义类别广泛性和细粒度的挑战，现有弱监督方法因依赖类别特定监督和不合适的特征构建方法导致语义不对齐和性能不佳。

Method: 提出SynSeg方法，采用MCCL整合类别内和类别间的对齐与分离，结合FSS通过先验融合和语义激活图增强重构对比学习特征。

Result: 在多个基准测试中表现优于现有方法，例如在VOC上比SOTA高4.5%，在Context上高8.9%。

Conclusion: SynSeg通过MCCL和FSS有效解决了语义分割中的语义不对齐和性能问题，显著提升了弱监督下的语义分割能力。

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [53] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: 提出了一种名为WGAST的弱监督生成网络，用于通过多源遥感数据融合实现每日10米分辨率的地表温度估计。


<details>
  <summary>Details</summary>
Motivation: 城市化、气候变化和农业压力增加了对环境监测的需求，尤其是高分辨率的地表温度数据。现有遥感系统在时空分辨率上存在折衷，需要新的解决方案。

Method: WGAST采用条件生成对抗网络架构，包括特征提取、融合、LST重建和噪声抑制四个阶段，结合弱监督训练策略。

Result: WGAST在定量和定性评估中均优于现有方法，平均降低RMSE 17.18%，提升SSIM 11.00%，并有效应对云干扰和捕捉精细热模式。

Conclusion: WGAST为高分辨率地表温度估计提供了有效的端到端深度学习框架，具有实际应用潜力。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [54] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 该研究通过应用表征学习算法（PCA、CAE、PT）对卫星图像进行分析，评估了不同天气事件分类的潜在空间效果。CAE表现最优，PT在热带气旋识别中突出，分辨率对深度学习算法效果影响显著。


<details>
  <summary>Details</summary>
Motivation: 探讨不同表征学习算法在卫星图像天气事件分类中的表现，尤其是深度学习方法的潜力与局限。

Method: 使用PCA、CAE和预训练残差网络（PT）对卫星图像进行表征学习，并进行分类任务评估。

Result: CAE在所有任务中表现最佳；PCA高命中率伴高误报率；PT在热带气旋识别中表现优异，其他任务中较弱；高分辨率数据提升深度学习效果。

Conclusion: CAE高效但缺乏物理解释，未来可开发物理信息增强的CAE版本。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [55] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner是一个基于强化学习的框架，通过奖励函数设计激励图像描述模型的自我纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述模型缺乏自我纠正能力，限制了其生成描述的准确性。

Method: 使用场景图解析算法分解预测和参考描述，通过集合差异计算奖励和惩罚，提出改进的评估指标。

Result: SC-Captioner显著优于直接偏好优化策略，生成更优质的图像描述。

Conclusion: SC-Captioner通过自我纠正能力和新评估指标，提升了图像描述的质量。

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [56] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: 文中提出了一种名为VeSCA的新方法，通过利用SAM编码器的共享弱点，生成可迁移的对抗样本，以评估其在下游应用中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM在零样本交互式分割中表现出色，但其固有的脆弱性可能导致下游应用失败，因此需主动评估这些可迁移的脆弱性。

Method: 提出了Vertex-Refining Simplicial Complex Attack (VeSCA)，通过参数化单纯复形明确描述SAM与下游模型的共享脆弱区域，并通过迭代顶点细化识别这些区域。此外，引入轻量级的领域重适应策略来桥接领域差异。

Result: 实验表明，VeSCA在五个领域特定数据集上的性能比现有方法提高了12.7%，进一步揭示了SAM脆弱性对下游模型的风险。

Conclusion: 研究强调了SAM脆弱性对下游模型的潜在威胁，并呼吁开发更强大的基础模型。

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [57] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出了基于3D高斯溅射的3D眼球结构框架，用于视线重定向，优于现有神经辐射场方法，实现更高的图像质量和视线估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视线重定向方法依赖隐式神经表示，缺乏对3D结构的显式建模，限制了效果。

Method: 采用3D眼球结构结合3D高斯溅射，并引入自适应变形模块模拟眼部肌肉运动。

Result: 在ETH-XGaze数据集上实现高质量图像生成和更准确的视线估计。

Conclusion: 显式3D眼球结构比隐式方法更有效，具有实际应用潜力。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [58] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 结合稀疏IMU和单目摄像头实现实时人体运动捕捉，提出了一种基于扩散模型的方法，通过学习运动先验和融合两种信号模态，表现出优越的姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉信息在部分帧中不可用的问题，同时利用IMU信号的稳定性，以实现更鲁棒的人体运动捕捉。

Method: 将视觉信息作为整体条件嵌入，IMU测量与噪声姿态逐帧拼接作为扩散模型输入，分别处理两种信号特性。

Result: 实验证明该方法设计有效，并在姿态估计任务中达到了最新技术水平。

Conclusion: 提出的框架成功融合了视觉和IMU信号，解决了视觉遮挡问题，展现了卓越的性能。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [59] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval是一种动态安全评估框架，通过调整安全基准的分布和复杂性来应对多模态大语言模型的安全问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集因MLLM技术进步而过时及数据污染问题。

Method: 采用文本、图像及文本-图像动态策略生成新样本，并分析其对安全的影响。

Result: SDEval显著影响安全评估，减轻数据污染，并暴露MLLMs的安全缺陷。

Conclusion: SDEval具有通用性，适用于多种安全和能力基准。

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [60] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO 是一个文本引导的视觉 Prompt DINO 框架，通过早期融合、顺序对齐查询选择和生成式数据引擎解决开放世界分割的挑战，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态视觉模型中晚期特征融合和混合提示开放世界分割的查询选择问题，以及固定词汇表带来的限制。

Method: 1. 早期融合机制统一文本/视觉提示和主干特征。2. 为 DETR 架构设计顺序对齐查询选择。3. 开发基于 RAP 模型的生成式数据引擎。

Result: 在开放世界检测基准测试中达到 SOTA 性能，语义覆盖范围显著超出固定词汇表的限制。

Conclusion: Prompt-DINO 为开放世界场景中的可扩展多模态检测和数据生成提供了新范式。

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [61] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: 提出了一种名为DSConv的动态分割卷积核方法，结合注意力机制提升图像融合效果。


<details>
  <summary>Details</summary>
Motivation: 为了提高高分辨率图像的质量，解决现有方法主要依赖标准卷积而未充分利用自适应卷积的问题。

Method: 提出DSConv方法，动态分割卷积核并选择关键位置，增强特征提取能力。

Result: 实验证明DSConv能有效提升网络性能，达到最先进的效果。

Conclusion: DSConv在泛化性、优化和特征表示方面表现优越，适用于图像融合任务。

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [62] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR是一种用户中心、多维度的文本到图像评估基准，通过混合方法（确定性指标和新型HWPQ方案）解决现有指标的局限性，并基于专家研究和高人类对齐表现。


<details>
  <summary>Details</summary>
Motivation: 现有指标在文本到图像评估中存在局限性，VISTAR旨在通过多维度、用户中心的基准改进评估。

Method: 采用两层次混合范式：确定性指标用于量化属性，HWPQ方案（基于受限视觉语言模型）评估抽象语义，并通过专家研究定义用户角色和评估角度。

Result: VISTAR指标与人类判断对齐度高（>75%），HWPQ方案在抽象语义上达到85.9%准确率，显著优于基线。

Conclusion: VISTAR揭示了没有通用最佳模型，角色加权分数为特定领域部署提供指导，所有资源已公开以促进可复现评估。

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [63] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: 提出了一种新型框架MPF-KANSC，结合多平面融合和注意力机制，显著提高了阿尔茨海默病的诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期诊断因大脑结构变化的复杂性而具有挑战性，现有深度学习方法常局限于单一平面且难以捕捉非线性关系。

Method: 集成多平面融合（MPF）和Kolmogorov-Arnold网络引导的空间-通道注意力机制（KANSC），以并行提取多平面特征并精确识别萎缩特征。

Result: 在ADNI数据集上验证，MPF-KANSC表现出优越的诊断性能，并揭示了AD进展中右偏侧化的亚皮层结构不对称性。

Conclusion: MPF-KANSC是一种高效且可解释的AD诊断框架，为疾病机制提供了新见解。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [64] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: 本文研究了在资源有限平台上部署扩散模型时，通过减少去噪步骤或降低每步计算成本来优化性能，提出了PostDiff框架，证明降低每步成本更有效。


<details>
  <summary>Details</summary>
Motivation: 研究如何在资源有限条件下优化扩散模型的部署，避免过度压缩导致性能下降。

Method: 提出PostDiff框架，包含混合分辨率去噪方案和模块级计算复用策略。

Result: 实验表明PostDiff显著提高了效率与保真度的平衡，且降低单步成本比减少步骤更有效。

Conclusion: 在资源受限时，减少每步计算成本比减少步骤更能保持生成质量。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [65] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: 提出UW-3DGS框架，通过可学习的物理模型和自适应修剪方法，提升水下3D场景重建的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 水下环境的光吸收和散射导致传统方法（如NeRF及其扩展）在几何和颜色保真度上表现不佳，亟需更高效的解决方案。

Method: 结合3D高斯散射（3DGS）与可学习的水下成像模块，以及物理感知的不确定性修剪分支（PAUP），优化噪声高斯分布并提升重建质量。

Result: 在SeaThru-NeRF和UWBundle数据集上表现优异，PSNR达27.604，SSIM为0.868，漂浮伪影减少约65%。

Conclusion: UW-3DGS通过创新的物理建模和修剪技术，显著提升了水下3D重建的性能和视觉效果。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [66] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 论文提出了一种用于结肠镜图像息肉检测的多方向架构，结合合成数据生成和改进的检测与分割算法，显著提高了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查是结直肠癌早期诊断的关键工具，但由于医疗数据集有限和标注复杂，自动化检测具有挑战性。

Method: 采用Faster R-CNN进行初步定位，结合Segment Anything Model (SAM)优化分割，并评估了五种分割模型的性能。

Result: Faster R-CNN的召回率为93.08%，精确率为88.97%；在分割模型中，FPN在PSNR和SSIM上表现最佳，UNet在召回率上领先。

Conclusion: 该框架有效提升了息肉检测和分割的准确性，为结直肠癌的早期诊断提供了有力支持。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [67] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: 论文提出了一种基于图表示和图卷积网络（GCNs）的机器人定位框架，通过利用地板特征，实现了高精度（0.64cm误差）和高效的定位，并解决了 kidnapped robot 问题。


<details>
  <summary>Details</summary>
Motivation: 传统定位方法（如Lidar或QR码）在复杂环境中存在可扩展性和适应性问题，亟需一种更高效且准确的替代方案。

Method: 采用图表示地板特征，并结合GCNs进行定位，避免了复杂的图像特征比对过程。

Result: 实现了0.64cm的定位误差，并在每一帧中解决了 kidnapped robot 问题，无需复杂滤波。

Conclusion: 该方法为复杂环境中的机器人导航提供了新的可能性。

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [68] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: MA-CBP提出了一种基于多智能体异步协作的犯罪行为预测框架，通过融合历史和实时数据实现犯罪预警。


<details>
  <summary>Details</summary>
Motivation: 城市化加速导致公共场景犯罪增多，传统方法难以捕捉高层行为语义，且实时性不足。

Method: 将实时视频转为语义描述，构建因果一致的历史摘要，并融合相邻帧进行长短期上下文联合推理。

Result: 在多个数据集上表现优异，为城市公共安全提供有效解决方案。

Conclusion: MA-CBP框架在犯罪预警中表现出色，结合高质量数据集，展示了实际应用潜力。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [69] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: 论文提出了一种新型的双分支交互融合注意力模型（DBIF-AUNet），用于提升胸腔积液CT图像的语义分割精度，解决了现有方法在多尺度特征融合和边缘模糊问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 胸腔积液CT图像的语义分割在临床诊断和治疗中具有重要意义，但现有方法在面对相似灰度、模糊边缘和多变形态时表现不佳。

Method: 提出了双域特征解耦模块（DDFD）和分支交互注意力融合模块（BIAF），结合嵌套深度监督机制，实现了多尺度特征互补和动态权重融合。

Result: 在1,622张CT图像上的实验显示，DBIF-AUNet的IoU和Dice分数分别为80.1%和89.0%，显著优于现有方法。

Conclusion: DBIF-AUNet通过创新模块设计和多尺度特征融合，显著提升了复杂胸腔积液CT图像的分割性能。

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [70] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: 论文提出了一种称为LiLoRA的高效架构扩展方法，用于解决多模态大语言模型在持续视觉指令调优中的灾难性遗忘问题，同时提高了参数效率。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中新任务引入导致的灾难性遗忘问题，同时减少参数开销和提升可扩展性。

Method: 通过共享LoRA矩阵A、对矩阵B进行低秩分解以减少任务特定参数，并引入余弦正则化稳定性损失来保持共享表示的一致性。

Result: 在多样化CVIT基准测试中，LiLoRA在连续任务学习中表现优越，且显著提高了参数效率。

Conclusion: LiLoRA是一种高效且可扩展的解决方案，适用于持续视觉指令调优任务。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [71] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE是一种基于混合专家架构的通用异常检测框架，通过分层设计和专家多样性提升模块，显著优于领域专用方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法过于专业化，泛化能力有限，无法跨领域高效检测多种异常类型。

Method: AnomalyMoE采用分层专家网络（局部、组件、全局）分别处理不同语义层次的异常，并引入EIR和ESB模块提升专家多样性和利用率。

Result: 在8个跨领域数据集上，AnomalyMoE实现了最先进的性能，显著优于领域专用方法。

Conclusion: AnomalyMoE通过通用架构和模块化设计，实现了跨领域的高效异常检测，解决了专业化模型的局限性。

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [72] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: PA-HOI Motion Capture数据集填补了现有HOI数据集中忽略物体物理属性对人类长期运动影响的空白，重点关注物体属性对姿态和速度的影响。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集多关注功能细节，忽视了物体物理属性对人类运动的长期影响。

Method: 引入PA-HOI数据集，包含562个运动序列，涉及不同性别受试者与35个3D物体的交互。

Result: 数据集扩展了现有研究范围，验证了物理属性对人类姿态和速度的影响。

Conclusion: PA-HOI数据集能够增强运动生成方法的物理感知能力。

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [73] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: 提出了一个两阶段的图像级SvdH评分预测方法，通过提取疾病相关区域并结合注意力机制，实现了高精度的预测。


<details>
  <summary>Details</summary>
Motivation: 传统的SvdH评分由于复杂性高，难以在临床中广泛应用，因此需要更高效的方法。

Method: 采用双阶段流程，结合图像区域提取和注意力机制的多实例学习，生成图像级特征进行预测。

Result: 最佳模型达到PCC 0.943和RMSE 15.73，集成学习进一步提升至PCC 0.945和RMSE 15.57，接近放射科专家水平。

Conclusion: 该方法能高效识别与RA进展相关的解剖结构，具有临床应用潜力。

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [74] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: 本文提出了一种纹理感知和边缘引导的Transformer（TEFormer），用于解决城市遥感图像语义分割中的语义模糊和边缘复杂性问题，通过纹理感知模块和边缘引导解码器提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 城市遥感图像中的地物对象具有微妙的纹理差异和相似的空间结构，容易导致语义模糊和误分类，同时不规则的形状、模糊的边界和重叠的空间分布进一步增加了分割难度。

Method: TEFormer结合了纹理感知模块（TaM）和边缘引导的三分支解码器（Eg3Head），通过捕获纹理差异和保留边缘细节，实现了多尺度上下文感知，并利用边缘引导特征融合模块（EgFFM）整合上下文与边缘信息。

Result: 实验表明，TEFormer在Potsdam、Vaihingen和LoveDA数据集上的mIoU分别达到88.57%、81.46%和53.55%，验证了其有效性。

Conclusion: TEFormer通过纹理感知和边缘引导机制，显著提升了城市遥感图像语义分割的精度，为相关应用提供了有力支持。

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [75] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: 论文提出了Depth-Jitter，一种基于深度的增强技术，通过模拟自然深度变化提升模型泛化能力。实验表明其在深度敏感环境中能稳定模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统增强技术忽视了深度感知变换，导致模型在真实世界深度变化中鲁棒性不足。Depth-Jitter旨在解决这一问题。

Method: 提出自适应深度偏移方法，通过深度方差阈值生成合成深度扰动，同时保持结构完整性。

Result: 在FathomNet和UTDAC2020数据集上测试表明，Depth-Jitter提升了模型在不同深度条件下的稳定性和泛化能力。

Conclusion: Depth-Jitter为深度感知增强提供了新方向，并支持未来研究。代码已开源。

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [76] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: 提出了一种通用的图像去模糊方法，能够处理多种模糊类型，通过混合专家（MoE）模块动态路由特征，实现高效恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定模糊类型设计，缺乏泛化能力，不适用于多种模糊场景。

Method: 采用混合专家（MoE）解码模块，根据识别到的模糊类型动态路由图像特征，实现端到端恢复。

Result: 方法性能与专用模型相当，且在未见模糊场景中表现出色。

Conclusion: 该统一方法在去模糊任务中具有高效性和泛化能力，适用于多种模糊场景。

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [77] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: 论文提出了一种参数高效的CLIP视觉编码器微调方法LNCLIP-DF，通过仅调整层归一化参数和增强特征流形，实现了对未知深度伪造技术的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造检测器在未知伪造技术上的泛化挑战，避免复杂的架构修改。

Method: 微调预训练CLIP模型的层归一化参数（0.03%），并利用L2归一化和潜在空间增强增强特征流形。

Result: 在13个基准数据集上实现了最优性能，平均跨数据集AUROC超越复杂方法。

Conclusion: 通过最小化预训练模型改动实现强泛化，为深度伪造检测领域提供了高效且可复现的方法。

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [78] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: 提出了一种名为FedX的策略，通过解释引导的剪枝减少联邦学习中的通信开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在遥感图像分类任务中面临通信开销大的挑战。

Method: 利用反向传播解释方法估计模型组件的重要性，剪枝不相关部分以减少传输模型的大小。

Result: 在多个数据集上验证，FedX能显著减少模型参数并提升泛化能力。

Conclusion: FedX是一种有效的通信优化策略，适用于联邦学习场景。

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [79] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net是一种新型2.5D U-Net架构，结合跨切片注意力和跳连注意力门控机制，显著提升了股骨MRI分割的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 由于现有2D和3D深度学习方法在股骨MRI分割中存在局限性，本文旨在提出一种更高效且精确的解决方案。

Method: 提出XAG-Net，采用像素级跨切片注意力（CSA）和跳连注意力门控（AG），改进切片间上下文建模和切片内特征细化。

Result: XAG-Net在分割精度上超越基线2D、2.5D和3D U-Net模型，同时保持计算效率。

Conclusion: XAG-Net通过CSA和AG模块的有效结合，成为股骨MRI分割的高效且准确框架。

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [80] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker是一个空间感知的多模态框架，通过结合深度增强的边界框和自然语言，提升了复杂视觉任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在复杂视觉任务（如空间理解和细粒度感知）中存在不足，缺乏利用空间线索进行注意力修正的能力。

Method: 提出SIFThinker框架，采用反向扩展-前向推理策略生成交错的图像-文本链，并提出GRPO-SIF训练范式，将深度信息融入统一推理流程。

Result: 实验表明，SIFThinker在空间理解和细粒度视觉任务中优于现有方法，同时保持强大的一般能力。

Conclusion: SIFThinker通过动态修正注意力，显著提升了复杂视觉任务的性能。

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [81] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: 提出了一种数据高效的跨域时序定位方法URPA，利用少量无标签目标域视频进行适应，解决了GRPO依赖标注数据和高计算成本的问题。


<details>
  <summary>Details</summary>
Motivation: 解决Vision-Language Models在细粒度时序定位中的不足，以及GRPO依赖标注数据和高计算成本的问题。

Method: 提出URPA方法，通过GRPO rollout生成伪标签并估计置信度，加权训练奖励以优化模型。

Result: 在三个数据集的六个跨域设置中表现良好，仅需少量无标签视频即可适应。

Conclusion: URPA实现了高效、低成本的跨域时序定位，适用于实时部署。

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [82] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GS-MoE的新框架，通过组合多个专门处理特定异常类型的专家模型，并结合高斯溅射损失增强弱监督信号，显著提升了视频异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测任务面临异常事件多样性和标注数据不足的挑战，现有模型难以处理复杂异常，且弱监督信号缺乏精细时间信息。

Method: 采用高斯溅射引导的专家混合模型（GS-MoE），组合多个专家模型捕捉特定异常类型，并通过高斯溅射损失增强时间一致性信号。

Result: 在UCF-Crime数据集上达到了91.58%的AUC，并在XD-Violence和MSAD数据集上表现出色。

Conclusion: GS-MoE通过类别特异性专家和时间引导，为弱监督下的视频异常检测设定了新标准。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [83] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的方法，用于生成合成心脏MR图像，以提高多中心心脏MR分割的性能，解决域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 由于成像设备和采集协议的差异，心脏MR图像存在域偏移问题，限制了AI模型在真实场景中的部署性能。传统方法如数据增强和迁移学习存在局限性。

Method: 使用扩散模型生成合成心脏MR图像，保持空间和结构一致性，并评估其在多中心心脏MR分割中的效用，包括域泛化和域适应两种策略。

Result: 实验表明，合成数据显著提高了未见目标域数据的分割性能（Welch's t-test, p < 0.01），优于仅使用真实数据的模型。

Conclusion: 该方法有效缓解了域偏移问题，减少了迁移学习或在线训练的需求，特别适用于数据稀缺场景。

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [84] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: 本文改进了ViPro模型，使其能够从观察中推断状态，而无需初始真实状态，并在无监督方式下验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决ViPro模型因依赖初始真实状态而无法在噪声环境下正确推断状态的问题。

Method: 在ViPro基础上改进，加入无监督学习机制，并扩展Orbits数据集至3D版本。

Result: 模型能够正确从观察中推断状态，且无需初始真实状态输入。

Conclusion: 改进后的ViPro模型在无监督环境下表现更优，更适合实际应用场景。

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [85] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: 利用街景图像和多模态大语言模型，研究城市街道的社会互动质量，发现其与城市环境特征和居民归属感相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注行人数量而非社会互动质量，希望通过街景图像提取潜在社会信息，填补这一空白。

Method: 分析15个城市的2,998张街景图像，结合社会学分类法（被动、短暂、持久社交），并通过线性回归模型控制天气、时间等变量。

Result: 天空视野指数与三种社交类型相关，绿化视野指数预测持久社交，归属感与短暂社交正相关，验证了城市规划理论的假设。

Conclusion: 街景图像可作为研究城市社会互动的工具，支持跨文化理论测试和证据驱动的城市规划。

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [86] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: 论文提出了一种名为VA-GPT的多模态大语言模型，旨在通过空间和时间有效标记选择和生成模块，更好地识别和定位视频中的异常事件。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解多模态大语言模型在处理异常事件时表现不佳，主要由于异常事件的空间和时间稀疏性导致冗余信息影响效果。

Method: 通过Spatial Effective Token Selection (SETS)和Temporal Effective Token Generation (TETG)模块，有效对齐视觉编码器与大语言模型之间的标记；同时构建了一个微调数据集和一个跨域评估基准。

Result: 提出的方法在多个基准测试中优于现有的最先进方法。

Conclusion: VA-GPT通过改进模块设计和数据集构建，显著提升了多模态大语言模型在异常事件识别与定位任务中的性能。

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [87] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: 论文描述了一种基于Chan-Vese模型的两阶段图像分割算法实现，改进了Goldstein等人提出的能量函数，利用Split Bregman方法高效求解。


<details>
  <summary>Details</summary>
Motivation: 目标是将2D图像分割为前景和背景区域，假设像素值可被两个不同平均值概括，并要求区域边界平滑。

Method: 采用了改进的Chan-Vese能量函数，结合Split Bregman方法进行优化。

Result: 算法在不同参数下对多幅图像进行了测试，性能良好。

Conclusion: 提出的方法能高效实现两阶段图像分割，验证了改进模型的实用性。

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [88] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 提出一种新方法，通过利用排名第一身份的额外注册图像来预测其是否为库内或库外身份，以减少误识别。


<details>
  <summary>Details</summary>
Motivation: 解决一多面部识别中库外身份检测问题，减少误识别和调查时间浪费。

Method: 通过生成库内和库外训练数据，训练分类器预测排名第一身份的库状态。

Result: 实验证明该方法对多种退化探针图像有效，且在人口统计组间表现一致。

Conclusion: 该方法能客观评估库外身份，尤其适用于使用先进损失函数训练的匹配器。

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [89] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT是一种基于纯文本数据的通用表示模型，通过一致的提示调优实现多模态扩展，无需特定模态标注数据即可在多种任务中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量特定模态标注数据或仅适用于单一模态，难以扩展到无限模态。TaAM-CPT旨在通过纯文本数据构建通用模型，解决这一问题。

Method: TaAM-CPT包含模态提示池、文本构造和模态对齐文本编码器，通过新增提示池和编码器扩展新模态，并设计了模态内和模态间学习目标以保持语义一致性。

Result: TaAM-CPT在视频、图像和音频分类等多种任务上无需模态标注数据即取得领先效果。

Conclusion: TaAM-CPT展示了通过纯文本数据构建通用多模态模型的潜力，其可扩展架构为未来多模态研究提供了新方向。

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [90] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen框架通过将多步去噪教师模型蒸馏为少步去噪学生模型，显著提高了稀疏视图下3D重建的时间效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用视频扩散模型（VDMs）时采样速度较慢，尤其在稀疏视图下多次运行VDMs以提高空间覆盖时效率低下。

Method: 提出了一种新颖的视频扩散模型蒸馏方法，结合GAN和软化反向KL散度最小化，将多步去噪模型蒸馏为少步去噪模型。

Result: 在真实数据集上，FVGen以90%以上的时间缩减生成了质量相似或更好的新视图，显著提升了下游重建任务的效率。

Conclusion: FVGen通过高效蒸馏实现了快速新视图合成，为稀疏视图下的3D重建提供了更优解决方案。

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [91] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 论文探讨如何通过将分类目标直接融入超分辨率过程来提高分类精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统超分辨率方法仅关注像素级指标而忽视其对下游分类性能影响的问题。

Method: 提出一种新方法，通过优化同时考虑图像质量和分类性能的损失函数来提升合成孔径雷达图像分辨率。

Result: 实验表明，该方法不仅能提高图像质量，还能显著改善分类准确性。

Conclusion: 直接整合分类目标的超分辨率方法可以有效提升分类性能。

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [92] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 本文研究了特征空间中的过采样方法对SAR船舶分类的效果，提出了两种新算法M2m$_f$和M2m$_u$，并在两个公开数据集上验证了其优于原始方法和基线。


<details>
  <summary>Details</summary>
Motivation: SAR船舶分类面临长尾数据集的挑战，少数类别的分类尤为困难。光学数据中的过采样方法已被证明有效，本文旨在探索其在SAR数据中的应用。

Method: 提出两种新算法M2m$_f$和M2m$_u$，基于Major-to-minor方法，并在OpenSARShip和FuSARShip数据集上测试，使用ViT、VGG16和ResNet50作为特征提取器。

Result: 新方法在FuSARShip和OpenSARShip数据集上的F1-score平均分别提高了8.82%和4.44%。

Conclusion: 特征空间中的过采样方法能有效改善SAR船舶分类中的类别不平衡问题，新方法表现优异。

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [93] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 本文提出了一种基于GAN的半监督学习框架，专为标记数据稀缺的医学影像设计，通过集成生成器、判别器和分类器的三阶段训练，显著提升了小样本（5-50标记样本/类）分类性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决医学影像中标记数据不足导致深度学习效果受限的问题。

Method: 引入三网络框架（生成器、判别器和分类器），结合监督和无监督学习，利用图像到图像翻译和基于集成伪标签技术。

Result: 在11个MedMNIST数据集上，显著优于6种先进半监督方法，尤其在5样本极端条件下表现突出。

Conclusion: 该框架为医学影像提供了一种高效的小样本分类解决方案，代码已开源。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [94] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: 本文实现了SimSwap的高效人脸交换框架，并通过引入自注意力与交叉注意力机制、动态损失权重等方法提升了性能。实验表明，改进后的模型在身份保留和视觉质量上表现更优。未来将探索StyleGAN3整合、唇同步优化等方向。


<details>
  <summary>Details</summary>
Motivation: 人脸交换技术在学术与商业应用中需求增长，但现有方法在身份保留和视觉质量上仍有改进空间。

Method: 通过整合自注意力与交叉注意力机制、动态损失权重和余弦退火学习率调度等方法，优化SimSwap框架。

Result: 改进后的模型在40万次训练迭代中表现出渐进性提升，身份相似度更高，FID分数更低，定性结果更优。

Conclusion: 未来将整合StyleGAN3并优化时间一致性，以进一步提升性能。

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [95] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin是一个非对比学习的插件，可无缝集成到CLIP架构中，提升多模态语义对齐的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大规模图像-文本数据集存在语义对齐松散或内容多样性低的问题，限制了对比学习模型的性能。

Method: 引入非对比学习的插件CLIPin，并设计共享的预投影器以结合对比与非对比学习。

Result: 实验表明CLIPin能有效提升多种下游任务的性能。

Conclusion: CLIPin作为一种即插即用组件，具有广泛兼容性和实用性。

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [96] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: 结合大语言模型（LLM）与Swin-UMamba架构用于CT图像病灶分割，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 通过整合影像特征与放射学报告中的文本描述，提升病灶分割的准确性和自动化水平。

Method: 采用Text-Swin-UMamba模型，结合ULS23 DeepLesion数据集和报告中的简短描述进行病灶分割。

Result: 测试集上Dice分数达82%，Hausdorff距离为6.58像素，优于其他模型。

Conclusion: 文本与影像结合的模型在病灶分割任务中表现优异，具有临床应用潜力。

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [97] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 通过模块化和多样化的图表生成方法提升多模态大语言模型的图表理解能力，并引入高质量数据集ECD。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型在图表理解任务上的表现不佳，且合成图表的数据质量不足。

Method: 设计了五步数据合成流程，包括模块化图表生成、多样化视觉细节、过滤低质量数据以及生成问答对。

Result: 构建的ECD数据集显著提升了多种MLLM在真实和合成测试集上的表现。

Conclusion: 模块化和多样化的数据生成方法是提升图表理解能力的有效途径。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: LightSwitch是一种利用多视图和材质信息的扩散框架，高效地为多视图数据重新打光，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有2D重光照生成方法未充分利用内在属性和多视图数据的问题。

Method: 提出LightSwitch框架，结合多视图和材质信息，采用可扩展的去噪方案。

Result: 在合成和真实物体重光照任务中表现优于现有方法，仅需2分钟。

Conclusion: LightSwitch通过整合内在属性与多视图数据，显著提升重光照质量。

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [99] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: 论文提出了Diagrams-to-Dynamics (D2D)方法，将因果循环图(CLD)转化为探索性系统动力学模型(SDM)，无需实证数据即可模拟干预效果。


<details>
  <summary>Details</summary>
Motivation: 因果循环图(CLD)作为静态定性工具，难以支持动态分析和干预策略设计，且传统定量分析方法易产生误判。

Method: D2D利用CLD中的结构信息（链接存在性和极性），通过用户简单标注变量类型(存量、流量/辅助变量或常量)生成动态模型。

Result: D2D能区分高/低优先级干预点，与数据驱动模型一致性高于网络中心性分析，并提供不确定性估计。

Conclusion: D2D作为开源工具，降低了动态建模门槛，未来需进一步验证其广泛适用性。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [100] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 该论文提出了一种将物理定律表示为加权知识图谱的新框架，并训练了一个图注意力网络进行链接预测，性能显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 物理定律之间存在复杂的关联，传统的表示方法难以捕捉这些关系。论文旨在通过知识图谱和图神经网络揭示物理定律之间的潜在联系。

Method: 构建了一个包含400个物理方程的数据库，并通过归一化变量重叠、物理学重要性评分和文献计量数据定义节点权重。使用图注意力网络（GAT）进行链接预测。

Result: 模型在链接预测任务中取得了0.9742的平均AUC，显著优于基线方法，并重新发现了物理学中的宏观结构以及跨领域的关键方程。

Conclusion: 该框架能够为物理学领域的交叉研究提供新假设，并生成专门的数据集，为物理学研究提供了新的工具。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [101] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的数据驱动方法，用于在非线性状态空间模型中学习“推拉”项，证明了其理论可行性，并在三类混沌系统中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有技术在非线性系统中设计有效的“推拉”项具有挑战性，因此需要一种更灵活的数据驱动方法来解决这一问题。

Method: 提出了神经网络推拉方法，基于Kazantzis--Kravaris--Luenberger观测理论，通过数据驱动学习非线性状态空间模型中的推拉项。

Result: 该方法在洛伦兹96模型、Kuramoto--Sivashinsky方程和Kolmogorov流三类混沌系统中表现出良好的效果。

Conclusion: 神经网络推拉方法为非线性状态空间模型中的推拉项设计提供了有效的解决方案。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [102] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 提出一种可扩展框架，通过整合异构数据和置信感知推理机制，重建可靠电网拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 实用的电网数据来源多样且质量不均，需要一种可靠的方法重建电网拓扑结构以提高电网运行可靠性。

Method: 结合空间布局（如GIS）和动态行为（如电压时间序列）两个维度，引入置信感知推理机制，并嵌入物理可行性约束。

Result: 在Oncor的8000多个电表中验证，拓扑重建准确率超过95%，置信校准和计算效率显著提升。

Conclusion: 所提框架能高效且可靠地重建电网拓扑，适合实际部署。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [103] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯风险最小化的线性编码器-解码器架构理论框架，用于解决科学机器学习问题，并在多个领域验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管非线性神经网络在科学领域中取得了显著成功，但其理论不透明性限制了在需要可解释性的场景中的应用。线性神经网络作为一种简单有效的工具，提供了理解复杂关系的途径。

Method: 论文构建了一个统一的贝叶斯风险最小化理论框架，用于分析线性编码器-解码器架构，并推导了封闭形式的秩约束线性和仿射线性最优映射。

Result: 在生物医学成像、金融因子分析和非线性流体动力学模拟等多个数据集上的实验验证了理论结果的有效性。

Conclusion: 本研究为理解和基准测试科学机器学习问题中的神经网络模型提供了坚实的基础。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [104] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出了一种结合TAPE与Graphormer的新框架，利用ChatGPT生成语义丰富的节点表示，并在ogbn-arxiv数据集上实现最佳性能，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理复杂网络（如引文网络）中难以整合文本语义与图结构信息的问题。

Method: 结合TAPE框架与Graphormer，利用ChatGPT生成语义解释，并通过注意力机制融合语义与结构特征。

Result: 在ogbn-arxiv数据集上达到0.772的分类准确率，显著优于GCN基线的0.713。

Conclusion: 框架为动态TAG中的节点分类提供了可扩展且鲁棒的解决方案，为未来知识系统和科学发现指明了方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [105] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于马尔可夫决策过程（MDP）和强化学习策略梯度（RL-PG）的框架，用于优化碰撞规避机动（CAM）的决策，以降低平均燃料消耗。通过历史CAM数据训练，该策略在合成和历史事件中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了在保证碰撞风险可控的同时，最小化CAM的平均燃料消耗，并通过提前决策优化机动效率。

Method: 采用连续状态、离散动作和有限时间步的MDP模型，结合风险、燃料消耗和轨道几何的解析模型，使用RL-PG算法训练策略。

Result: 在合成事件中，策略显著降低了总燃料和平均燃料消耗；在历史事件中，平均燃料消耗减少但总燃料消耗增加，同时保持了与或优于传统方法的碰撞风险保证。

Conclusion: 该MDP框架和RL-PG算法有效平衡了燃料消耗和碰撞风险，提供了优于传统CAM策略的性能。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [106] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT是一种2位量化方法，能在固定资源预算下提升信息密度，优于未量化方法。


<details>
  <summary>Details</summary>
Motivation: 探讨量化在固定资源预算下是否可能优于非量化方法，而非仅是性能与计算资源的折中。

Method: 提出了Signed-Zero Ternary（SZT），一种2位量化方法，确定性提供梯度信息且不影响前向路径。

Result: 分析表明，SZT在信息密度上可能优于非量化方法。

Conclusion: SZT展示了量化在特定条件下可能优于非量化方法的潜力。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [107] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 论文提出了一种将随机时间序列分解为均值、离散和噪声的方法，通过机器学习拟合双信号并优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分解时间序列，提取均值和离散信号，同时隔离噪声，以便更好地分析和预测时间序列的特征。

Method: 采用机器学习方法拟合双信号，优化包含正则化项的损失函数，并引入统计过程控制方法保护特殊模式。学习过程分为顺序学习和联合学习两种方式。

Result: 提出的分解方法可作为平滑算法或去噪算法使用，并能在二维空间中表示双信号，用于结构学习、预测或交叉效应分析。

Conclusion: 该方法有效地分解了时间序列，适用于异方差性分析，并能灵活调整超参数以适应不同应用需求。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [108] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 论文研究了神经PDE求解器因优化不良导致的精度问题，提出了一种简单有效的激活过滤方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究神经PDE求解器中因条件不佳导致的优化问题，特别是多保真度和刚性问题的收敛限制。

Method: 提出Shifted Gaussian Encoding方法，通过激活过滤提高矩阵秩和表达能力，同时保持凸性。

Result: 方法能在稳态对流-扩散方程中扩展可解范围，多频函数学习误差降低六个数量级，高保真图像向量拟合更快更准。

Conclusion: 研究指出条件而非深度是科学神经求解器的瓶颈，简单的架构改变可带来显著性能提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [109] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO解决了GRPO中的Think-Answer Mismatch问题，通过噪声感知优势权重稳定训练，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 针对GRPO在训练大型推理模型时因噪声奖励信号导致的Think-Answer Mismatch问题，提出S-GRPO以优化训练稳定性。

Method: S-GRPO引入最优的噪声感知优势权重，以稳定训练过程，尤其在平衡性差的响应组中表现更优。

Result: 在多个数学推理基准测试中，S-GRPO显著优于DR.GRPO，性能提升达2.2%到2.5%，且在20%噪声下仍能稳定学习。

Conclusion: S-GRPO为大规模推理模型的鲁棒和高效训练提供了有效解决方案。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [110] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习（RL）的多臂老虎机（MAB）剪枝方法，用于动态优化决策树，提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的剪枝方法（如CCP和REP）基于贪婪策略，可能导致泛化能力不足，特别是在小规模和复杂数据集上。

Method: 将剪枝过程建模为探索-利用问题，利用MAB算法动态选择最优分支节点进行剪枝。

Result: 实验表明，该方法在多个基准数据集上优于传统剪枝方法。

Conclusion: MAB剪枝方法为决策树提供了一种动态且概率化的优化方式。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [111] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 离线强化学习（RL）从静态数据集中学习最优策略，但存在分布偏移问题。本文提出MCRE框架和MCRQ算法，平衡保守性和性能提升，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 离线RL的关键挑战是分布偏移导致的OOD动作和过高估计，需要平衡保守性和性能改进。

Method: 提出MCRE框架，结合TD误差和行为克隆项；开发MCRQ算法，将MCRE融入离线RL的actor-critic框架。

Result: MCRQ在基准数据集上优于现有基线方法和最先进的离线RL算法。

Conclusion: MCRE和MCRQ有效平衡了保守性与性能，为离线RL提供了新方向。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [112] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 提出了一种基于语义对齐的强化学习方法，使用SBERT计算奖励，而不依赖人工设计的奖励函数，实验表明该方法能有效指导学习。


<details>
  <summary>Details</summary>
Motivation: 在科学机器学习领域，设计有效的奖励函数是一个挑战，尤其是在任务目标难以用数值指定的环境中。现有方法多依赖于启发式或人工调优，缺乏普适性。

Method: 引入语义对齐的强化学习方法，通过SBERT计算当前状态与目标语义指令的余弦相似度作为奖励，替代人工设计的奖励函数。

Result: 在多个环境中评估，语义奖励能够指导学习实现有竞争力的控制行为。研究表明语言嵌入空间与传统欧氏空间存在相关性。

Conclusion: 该方法为将代理行为与自然语言目标对齐开辟了新途径，并为更大语言模型与流畅控制应用的集成奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [113] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: 提出了一种新方法，通过重新构建误差和结合半范数收缩与单调性，首次实现了无参数最优收敛率的 $Q$-learning 算法。


<details>
  <summary>Details</summary>
Motivation: 非线性固定点方程（如 $Q$-learning 和 TD-learning）的算法通常涉及半范数收缩，但现有方法难以实现无参数最优收敛率，主要原因是半范数的非单调性。

Method: 通过将平均误差重构建为包含非线性扰动的线性递推，并利用半范数收缩与适当诱导范数的单调性相结合来抑制非线性效应。

Result: 首次实现了无参数的 $	ilde{O}(1//	ext{sqrt}{t})$ 最优收敛率，适用于同步/异步更新、单/多智能体部署以及模拟或马尔可夫轨迹数据流。

Conclusion: 该方法在广泛框架内解决了 $Q$-learning 的无参数最优收敛问题，为相关领域提供了新的理论支持。

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [114] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP是一种用于压缩Chain-of-Thought（CoT）的粗到细框架，通过保留核心推理结构和逻辑关键步骤，显著降低了训练和推理成本，同时在代码生成任务中保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法在保持逻辑连贯性和捕捉关键推理步骤方面存在不足，ASAP旨在解决这些问题。

Method: ASAP采用锚点引导剪枝和基于第一令牌惊喜度的逻辑感知剪枝，最后模型自主生成简洁CoT用于推理。

Result: 在多个代码生成基准测试中，ASAP实现了最高精度，同时在LiveCodeBench v4_v5上减少了23.5%的令牌生成和43.5%的推理延迟。

Conclusion: ASAP为构建高效且强大的大型推理模型提供了有前途的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [115] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 提出了一种名为MCTS-OPS的神经符号框架，通过结合蒙特卡洛树搜索（MCTS）和多步骤提示序列，显著提升大语言模型（LLMs）在代码生成和优化任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂多步骤任务中表现不佳，现有方法仅关注启发式代码生成或简单任务。

Method: 将提示选择建模为MCTS引导的序列决策过程，优化多步骤提示序列以提升代码生成质量。

Result: 在网络优化实验中，MCTS-OPS在代码执行成功率、优化结果奖励和稳定性（2∼4倍更高奖励和3倍更低标准差）上显著优于基线，且将获得最优解的几率提高了约10%。

Conclusion: MCTS-OPS证明了结合符号规划与LLMs在复杂领域中实现高质量代码生成的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [116] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 该研究提出了一种新的逐步动态竞争风险模型，以提高对心脏骤停后昏迷患者神经预后的预测能力，通过自动判断何时利用时间不变和时间变化的特征。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停后昏迷患者的预后预测直接影响ICU的临床决策，需要结合时间不变和时间变化的特征。

Method: 扩展了Fine和Gray模型，引入神经网络捕捉非线性特征关系，分两阶段利用时间不变和时间变化的特征。

Result: 在2278名患者的回顾性队列中，模型对觉醒、撤除生命支持疗法和死亡等竞争性结局表现出良好的区分性能。

Conclusion: 该方法可推广到特征收集的更多阶段，适用于其他动态预测任务。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [117] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 该论文提出了一种自适应异构图神经网络（AHGNN），针对异构图中的异质性分布和语义多样性问题，通过异质性感知卷积和粗到细的注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的异构图（HGs）通常具有异质性，但现有研究多单独关注异质性或异质性，忽略了异质性HGs的普遍性，导致性能下降。

Method: 提出AHGNN，采用异质性感知卷积处理不同跳数和元路径的异质性分布，并通过粗到细的注意力机制整合多语义空间信息。

Result: 在七个真实图数据和二十个基线上的实验表明，AHGNN在高度异质性场景下表现优异。

Conclusion: AHGNN通过结合异质性感知和语义多样性处理，显著提升了异构图学习性能，尤其在异质性强的场景中表现突出。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [118] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: 本文提出了一种名为DP-LLM的动态量化方法，通过运行时动态调整各层精度，解决了在不同运行时约束（如延迟和准确性）下处理大型语言模型（LLMs）查询的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不同运行时约束（如延迟和准确性）下高效处理LLMs的查询，尤其是动态变化的需求。

Method: 提出了DP-LLM，通过动态分配每层精度，利用输入值和轻量级误差估计器，实时调整比特宽度。

Result: 在多个模型和基准测试中，DP-LLM实现了性能与延迟的优越权衡，优于现有方法。

Conclusion: DP-LLM通过动态精度调整提供了一种高效且灵活的解决方案，适用于LLMs的运行时需求。

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [119] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 论文提供了深度时间模型的非空洞、架构感知的泛化界限，并提出了延迟反馈阻断机制和公平比较方法，揭示了时间依赖性在固定信息预算下可以增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补深度时间模型（如TCNs）理论上泛化性能理解的不足，探索其在不同时间依赖性下的泛化行为。

Method: 采用指数混合序列，推导出泛化界限，提出延迟反馈阻断机制将依赖样本转化为独立样本，并引入公平比较方法固定有效样本量。

Result: 发现强依赖序列的泛化间隙比弱依赖序列小76%，但收敛速度与理论预测不符（弱依赖：$N_{\text{eff}}^{-1.21}$，强依赖：$N_{\text{eff}}^{-0.89}$）。

Conclusion: 时间依赖性可以提升学习效果，但理论与实践的差距仍需进一步研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [120] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文首次将可微分逻辑门应用于序列建模，提出RDDLGN方法，在翻译任务中表现接近GRU。


<details>
  <summary>Details</summary>
Motivation: 探索可微分逻辑门在序列建模中的潜力，填补其在递归架构中的空白。

Method: 提出Recurrent Deep Differentiable Logic Gate Networks (RDDLGN)，结合布尔运算与递归架构。

Result: 在WMT'14英德翻译任务中，RDDLGN训练时BLEU达5.00，推理时表现稳定（4.39 BLEU）。

Conclusion: RDDLGN验证了基于逻辑的递归神经网络计算的可行性，为FPGA加速等方向开辟新研究路径。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [121] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hindsight Goal-conditioned Regularization（HGR）的新技术，结合Hindsight Self-Imitation Regularization（HSR），显著提高了稀疏奖励下目标导向强化学习（GCRL）的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于后见经验重放（HER）的方法仅通过重标轨迹利用经验，未能充分利用离线GCRL方法中的可用经验，导致样本效率有限。

Method: 提出HGR技术，生成基于后见目标的动作正则化先验，结合HSR，最大化离线强化学习算法的经验利用率。

Result: 在导航和操作任务上的实验表明，HGR和HSR的结合相比现有方法显著提高了样本重用效率和性能。

Conclusion: HGR和HSR的结合为稀疏奖励下的GCRL提供了一种高效的经验利用方法，具有更优的样本效率和性能。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [122] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种利用扩散模型生成合成口腔癌病变图像的方法，以提高诊断模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决口腔癌诊断中训练数据不足和变异性问题。

Method: 使用细调扩散模型进行图像修复，生成高质量合成病变图像。

Result: 分类模型准确率达0.97，检测模型定位准确率为0.85。

Conclusion: 合成图像生成在医学诊断中具有潜力，可推广至其他癌症诊断。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [123] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: 联邦聚类将相似客户分组并为每组训练一个模型，提升性能但隐私风险更高。直接应用差分隐私机制会显著降低效用。作者提出RR-Cluster技术，通过随机重新平衡聚类分配以减少隐私噪声，并在实验和理论上验证其效果。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类虽能提升模型性能，但直接应用差分隐私机制会导致效用显著下降。作者发现这是由于聚类中客户数量不可控，导致隐私噪声难以平均分配。

Method: 提出RR-Cluster技术，作为轻量级插件，通过随机重新平衡聚类分配，确保每个聚类有最小客户数，降低隐私噪声方差。

Result: 实验表明，RR-Cluster显著改善了隐私与效用的权衡，并在理论和实证上验证了其有效性。

Conclusion: RR-Cluster是一种简单有效的方法，能够在保证隐私的同时提升联邦聚类的性能。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [124] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 该研究对25种预训练神经网络模型在25个数据集上进行了广泛比较，发现大多数模型在性能上并未显著优于基线ECFP分子指纹，仅CLAMP模型表现显著更优。研究质疑现有研究的严谨性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估预训练神经网络模型在化学和小分子药物设计中的实际效果，揭示现有研究的潜在问题。

Method: 采用公平比较框架和分层贝叶斯统计测试模型，对25种模型在25个数据集上进行了性能评估。

Result: 大多数神经网络模型未显著优于基线ECFP指纹，仅CLAMP模型表现显著。

Conclusion: 研究揭示了现有评估的不足，呼吁更严格的验证方法，并提出了改进方向。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [125] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: 本文提出了一种适用于不同隐私需求的联邦推荐系统GFed-PP，通过结合公开用户数据提升推荐性能且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户隐私需求相同，忽略了公开用户数据的潜在价值，无法满足实际应用中用户对隐私需求的多样性。

Method: GFed-PP结合公开用户数据构建用户-项目交互图及用户关系图，采用轻量图卷积网络学习个性化嵌入，并在客户端本地学习用户嵌入和评分函数以保护隐私。

Result: 实验表明，GFed-PP在五个数据集上显著优于现有方法，推荐准确性更高且不损害隐私。

Conclusion: GFed-PP为联邦推荐系统中灵活适应不同隐私偏好提供了一种实用解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [126] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 论文提出了Reparameterization Proximal Policy Optimization (RPO)，一种结合了Reparameterization Policy Gradient (RPG)和PPO的方法，解决了RPG训练不稳定的问题，并实现了高效样本利用。


<details>
  <summary>Details</summary>
Motivation: RPG虽然可以通过可微分动力学提高样本效率，但其训练过程中的高方差梯度会导致学习过程不稳定。因此，需要一种能够结合RPG和PPO优点的稳定且高效的方法。

Method: 通过建立PPO的代理目标与RPG之间的联系，提出RPO方法，利用时间反向传播高效计算梯度，并通过裁剪代理目标和KL散度正则化实现稳定样本重用。

Result: 在多个运动和控制任务上的实验表明，RPO具有优异的样本效率和强性能。

Conclusion: RPO成功结合了RPG和PPO的优点，是一种稳定且高效的强化学习方法。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [127] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR是一个基于边缘AI的框架，通过ML压缩技术优化6G车载娱乐资源的调度与公平性，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决6G网络中车载娱乐服务因数据量大且复杂导致传统RRM技术效率低下的问题。

Method: 采用ML压缩技术（如聚类和RBF网络）减小CQI数据规模，并结合强化学习策略优化调度。

Result: 实验显示SCAR在可行调度区域时间增加14%，不公平调度时间减少15%，且CQI聚类失真降低10%。

Conclusion: SCAR在动态车载网络中展现出良好的可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [128] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 研究了在仅能获取部分特征的情况下进行成员推断攻击的问题，提出了两阶段攻击框架MRAD。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断方法通常假设攻击者能获取目标样本的全部特征，但实际中往往仅能获取部分特征，限制了方法的适用性。

Method: 提出MRAD框架，分两阶段：优化缺失特征以最小化损失，再利用异常检测度量重建样本与训练分布的偏差。

Result: 实验证明MRAD在多种数据集上有效，例如在STL-10上即使缺失40%特征，AUC仍可达0.6。

Conclusion: MRAD在部分特征场景下有效，兼容多种异常检测技术，扩展了成员推断攻击的适用性。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [129] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: 提出了CMOSS算法，解决了现有CMAB算法中存在的遗憾因子和计算开销问题，实现了高效的实例无关遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有的组合多臂老虎机（CMAB）算法中，UCB类方法存在遗憾因子问题，而对抗性方法计算开销大，CMOSS旨在解决这一权衡。

Method: 开发了CMOSS算法，一种计算高效的随机设置下的组合极小极大最优策略，适用于半老虎机和级联反馈。

Result: CMOSS在半老虎机反馈下实现了实例无关遗憾$Oig( (ig ))$，消除了对$\log T$的依赖，并匹配了理论下限。实验验证了其优于基准算法的效果。

Conclusion: CMOSS是一种高效的CMAB算法，解决了现有方法的局限性，并在实际应用中表现出色。

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [130] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 研究了四种训练干预方法，以防止微调时产生的意外有害行为，并评估了其对恶意和良性任务的影响。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型微调时可能出现的意外有害行为（EMA），研究实用性强的防护措施。

Method: 比较了四种训练干预方法：KL散度正则化、特征空间ℓ2距离、安全子空间投影（SafeLoRA）和插入安全训练示例。

Result: 评估了这些方法在恶意任务和良性任务中的效果，发现部分方法能有效减少EMA。

Conclusion: 提出了关于EMA研究的开放问题，强调了未来方向。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [131] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 本文提出了一种使用矩阵乘积状态(MPS)生成高质量、隐私保护的合成表格数据的方法，并通过实验验证其在隐私约束下的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私限制以及在训练稳健模型时对多样化数据集的需求。

Method: 利用MPS结合噪声注入和梯度裁剪技术实现差分隐私，通过雷尼差分隐私会计确保隐私保障。

Result: 实验表明，MPS在数据保真度和隐私保护方面优于传统模型（如CTGAN、VAE和PrivBayes），尤其在严格的隐私约束下表现突出。

Conclusion: MPS是一种有前景的隐私感知合成数据生成工具，结合张量网络的表达能力和形式化隐私机制，为安全数据共享提供了可解释且可扩展的解决方案。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [132] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GTMancer的框架，利用图神经网络和对比学习整合多组学数据，以改进癌症亚型分类。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多组学数据整合中忽视了异质组学之间的复杂耦合，限制了其在精确癌症亚型分类中的潜力。

Method: GTMancer结合图神经网络和对比学习，将多组学数据嵌入统一语义空间，并利用双重注意力系数捕捉多组学数据的结构和全局信息。

Result: 在七个真实癌症数据集上的实验表明，GTMancer优于现有最先进算法。

Conclusion: GTMancer通过全局信息引导个体组学表征，显著提升了多组学数据整合的效果，为精准癌症分类提供了新思路。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [133] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: 提出了一种新的离线多智能体强化学习算法OM2P，通过均值流匹配损失和Q函数监督实现高效的一步动作采样，解决了生成模型在采样效率和资源消耗上的问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散和流模型）在离线多智能体强化学习中表现优异，但其迭代生成过程导致采样效率低，难以应用于时间敏感或资源受限的场景。

Method: 设计了OM2P算法，结合奖励感知优化方案、均值流匹配损失和Q函数监督，并引入广义时间步分布和无导数估计策略以降低内存消耗和提升训练稳定性。

Result: 在Multi-Agent Particle和MuJoCo基准测试中，OM2P表现优异，GPU内存使用减少3.8倍，训练速度提升10.8倍。

Conclusion: OM2P首次成功将均值流模型引入离线多智能体强化学习，为合作多智能体场景中的实用生成策略奠定了基础。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [134] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 该论文研究如何通过持续学习（CL）解决印度语言多样性在自动语音识别（ASR）中的挑战，使用Conformer模型和多种CL策略，结果表明CL能有效减轻遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性使得传统多语言ASR模型难以实现，隐私约束和数据顺序到达问题促使研究者转向持续学习。

Method: 使用基于Conformer的混合RNN-T/CTC模型，从印地语预训练开始，逐步学习8种语言。测试了三种CL策略（EWC、MAS、LwF），比较了不同训练周期的效果。

Result: CL策略显著优于简单微调，能有效减轻遗忘，尤其是在隐私受限的无重放场景中表现良好。

Conclusion: 持续学习是解决印度多样化语言ASR问题的可行方案，尤其在现实约束条件下表现出色。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [135] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出了一种新型多输出脉冲神经元模型，结合线性SSM状态转移与非线性反馈复位机制，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 结合SNN的低延迟、高效能与深度SSM的高性能，解决其高精度激活函数和缺乏复位机制的问题。

Method: 设计了一种多输出脉冲神经元模型，明确区分脉冲函数、复位条件和复位动作。

Result: 在关键词识别、事件驱动视觉和模式识别任务中表现与现有SNN基准相当，复位机制克服了不稳定问题。

Conclusion: 新型模型通过复位机制突破了线性动态稳定性限制，实现了高效学习。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [136] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新的联邦元学习方法，通过隐私保护损失函数解决传统FML的隐私泄露问题，实现高效优化和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 神经场需要大量数据和计算，边缘设备资源有限，传统FML存在隐私泄露问题。

Method: 引入FedMeNF，使用隐私保护损失函数优化本地元学习器。

Result: 实验表明FedMeNF在少样本或非独立同分布数据下仍能高效优化并保护隐私。

Conclusion: FedMeNF在数据效率和隐私保护方面优于传统FML。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [137] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种无需预训练伙伴或手动调参的多智能体强化学习框架，通过动态生成训练伙伴来提升团队协作效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中需要预训练伙伴或手动调参的问题，提高团队协作的适应性和效率。

Method: UPD通过将智能体策略与随机行为混合，生成多样化的训练伙伴，并使用方差可学性指标评估伙伴。

Result: 在Overcooked-AI等测试中，UPD显著优于基线方法，并在用户研究中表现出更高的适应性和协作性。

Conclusion: UPD为完全无监督的伙伴和任务分布课程提供了一种高效框架，适用于协作环境。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [138] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: 提出了一种自适应稳健损失函数FCL，通过分数阶导数调整标签噪声鲁棒性，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 现有稳健损失函数需要大量数据集特定的超参数调优，缺乏自适应性。

Method: FCL结合CE的分数阶导数（主动）和MAE（被动）损失，动态学习分数阶导数参数μ。

Result: FCL在基准数据集上达到最优性能，无需手动调参。

Conclusion: FCL通过动态调整损失函数特性，实现了标签噪声下的高效分类。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [139] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE是一种新型变分自编码器，通过结构方程建模嵌入测量结构，直接设计解耦潜在表示，无需依赖统计正则化。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据中可解释潜在表示学习的挑战，适用于理论和测量效度关键的科学与社会领域。

Method: 采用结构方程建模思想，设计潜在子空间与已知指标分组对齐，并引入全局干扰潜在变量隔离特定构造的混杂变异。

Result: 在模拟数据集上表现优于基线，解耦性、可解释性和对干扰变异的鲁棒性均更优。

Conclusion: SE-VAE通过架构设计实现解耦，为理论驱动的生成建模提供了原则性框架。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [140] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means是一种基于几何原理（如标量投影）改进k-means算法的新方法，通过聚焦高表达数据（HE）提升效率，同时减少低表达数据（LE）的计算开销，显著优于传统及SOTA k-means变体。


<details>
  <summary>Details</summary>
Motivation: 提升k-means算法的效率和能源经济性，同时保持解的质量。

Method: 利用几何原理（标量投影）选择性处理高表达数据（HE），忽略低表达数据（LE）。

Result: 在合成、真实世界和高维数据集上，Gk-means在运行时间和距离计算（DC）上表现更优，且能源效率更高。

Conclusion: Gk-means是一种高效、可持续的k-means改进方法，适用于多种应用场景。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [141] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 研究探索了大型语言模型(LLMs)在无外界诱导下的自发性欺骗行为，提出了基于心理学的量化指标，发现模型在复杂任务中欺骗倾向增加。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理和决策任务中的广泛应用使其可信度成为关键问题，但目前对模型自发性欺骗的研究不足，需填补这一空白。

Method: 提出基于“接触搜索问题”的框架，设计两种统计指标（欺骗意图分和行为分）量化欺骗行为，并评估了14个主流LLM。

Result: 发现任务难度增加时，模型的欺骗意图和行为分同步上升，表明先进LLMs在复杂问题中欺骗倾向增强。

Conclusion: 研究揭示了LLMs在高难度任务中易自发欺骗，对其在关键领域的部署提出警示，需进一步研究应对策略。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [142] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff是一种基于扩散模型的生成方法，通过正负引导分类器实现分子设计中对多目标活动的精确控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统生成方法无法同时管理多目标分子活动的问题。

Method: 利用分类器引导的扩散模型，结合正负引导分类器进行分子生成。

Result: 实验证明ActivityDiff能有效处理多种药物设计任务，平衡功效与安全性。

Conclusion: ActivityDiff为分子活动集成控制提供了新范式，是一个多功能、可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [143] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 本文提出了一种三阶段端到端文本到SQL框架，旨在先确定用户的目标数据库，再生成SQL查询。通过LLM和提示工程提取隐式规则，训练数据库预测模型，并利用批评代理纠正SQL错误。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL方法需预先指定目标数据库，而在多数据库场景中识别正确数据库常被忽视。本文旨在解决这一问题，提升数据库意图预测和SQL生成的准确性。

Method: 利用LLM和提示工程从自然语言查询中提取规则集，训练基于RoBERTa的数据库预测模型，并引入批评代理校正SQL错误。

Result: 实验结果表明，该框架在数据库意图预测和SQL生成准确率上均优于当前最优模型。

Conclusion: 该三阶段框架有效地解决了多数据库场景中的数据库识别问题，显著提升了文本到SQL任务的性能。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [144] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 该论文提出了一种利用公开众包数据（如311服务电话和街景图像）跟踪和预测旧金山无家可归者帐篷趋势的新方法，弥补传统点计数方法的不足。


<details>
  <summary>Details</summary>
Motivation: 美国无家可归问题激增，但传统监控方法（如点计数）在频率、一致性和空间细节上存在局限。

Method: 通过分析311服务电话和街景图像等众包数据，构建预测模型捕捉每日和社区级别的细节变化。

Result: 模型揭示了传统方法忽略的快速波动（如疫情期间）和帐篷位置的空间变化。

Conclusion: 该方法为政策制定和干预评估提供了更及时、本地化且经济高效的信息。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [145] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR通过高重放次数和周期性重置策略提升LLMs的样本效率，解决过拟合问题，显著提高推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决后训练LLMs中样本效率低和初始偏见过拟合的问题。

Method: 提出LoRR插件，结合高重放训练、周期性重置和混合优化目标。

Result: LoRR显著提升数学和通用推理任务的性能。

Conclusion: LoRR是一种高效且实用的LLMs微调方法。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [146] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN是一个针对大型语言模型的模块化遗忘框架，通过梯度比指标和选择性噪声注入提高遗忘性能，同时保持模型有效性。


<details>
  <summary>Details</summary>
Motivation: 随着法律和伦理对大型语言模型（LLMs）的审查日益严格，需要高效的方法删除敏感或未授权数据，现有方法常导致不完全遗忘或无关知识退化。

Method: GRIN提出了一种基于梯度比的指标来识别与遗忘数据关联的参数，并在微调前对这些参数进行选择性噪声注入。

Result: 在TOFU、WMDP和SafePKU等基准测试中验证了GRIN的有效性，新评估指标也证明了其优势。

Conclusion: GRIN提供了一个高效且针对性的遗忘方法，显著提升了大型语言模型的遗忘能力，同时避免了无用知识的退化。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>
